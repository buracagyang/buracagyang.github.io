---
title: 【Graph Embedding】DeepWalk
toc: true
comments: true
tags:
  - 算法备忘
  - 图计算
  - Embedding
date: 2019-12-26 16:16:26
---

关于Graph Embedding系列的论文翻译解读文章：

[【Graph Embedding】DeepWalk](https://buracagyang.github.io/2019/12/26/graph-embedding-deepwalk/)

[【Graph Embedding】line](https://buracagyang.github.io/2019/12/21/graph-embedding-line/)

[【Graph Embedding】node2Vec](https://buracagyang.github.io/2019/12/26/graph-embedding-node2vec/)

[【Graph Embedding】SDNE](https://buracagyang.github.io/2020/01/14/graph-embedding-SDNE/)

[【Graph Embedding】struc2vec](https://buracagyang.github.io/2020/01/13/graph-embedding-struc2vec/)



**参考资料**

paper: http://www.perozzi.net/publications/14_kdd_deepwalk.pdf

code: https://github.com/phanein/deepwalk

<!--more-->



# 1. 介绍

![这里写图片描述](figure1.png)

DeepWalk将图形作为输入，将生成的潜在表示作为输出。应用我们的结果深入研究空手道网络的方法如图1。该图通常由力导向的布局表示（1a）。图1b显示了我们方法两个潜在维度的输出。除了惊人的相似之外，我们注意到(1b)的线性可分部分对应于通过输入图(1a)中的模块最大化发现的集群(以顶点颜色显示)。为了展示DeepWalk在现实世界场景中的潜力，我们在具有挑战性的多标签上评估其性能，大型异构图中的网络分类问题。在关系分类问题中，特征向量违反了传统的i.i.d.假设（独立同分布假设）。 为了解决这个问题，通常使用近似值推理技术[32]以利用依赖信息改善分类效果。 通过学习与标签无关的图形表示，我们与这些方法保持距离。表示质量没有影响的选择标记顶点,所以他们之间可以共享任务。

在创造社交维度方面，DeepWalk的表现优于其他潜在的表现方法[39，41]，尤其是当标记的节点很少。 我们使用非常简单的线性分类器的表现非常出色（例如逻辑回归）。 我们的陈述是一般性的，并且可以与任何分类方法（包括迭代推理方法）。 DeepWalk实现了所有这些目标同时是可以并行化的在线算法。

我们的贡献如下：

+ 我们引入深度学习作为分析图形的工具，以构建适合于统计建模的健壮表示。DeepWalk学习短随机漫步中出现的结构规律。

+ 我们广泛地评估了我们在几个社交网络上的多标签分类任务的表现。我们发现，在标签稀疏性存在的情况下，分类性能显著提高，并得到了改进Micro-F1的5%-10%，我们考虑的最稀疏的问题。在某些情况下，即使训练数据减少60%，DeepWalk的表现也能超越竞争对手。

+ 我们通过构建web级图的表示来演示我们的算法的可伸缩性使用并行实现。此外，我们还描述了构建我们的方法的流版本所需的最小变动。

  

# 2. 问题定义

我们考虑将社交网络成员划分为一个或多个类别的分类问题。定义$G=(V,E)$，$V$是网络节点的集合，$E$是节点之间的边的集合。$E \subseteq (V \times V)$。给定一个有标签的社交网络$G_L=(V,E,X,Y)$,它有属性$X \in \Bbb R^{|V| \times S}$，其中$S$是每个属性的特征空间大小，$Y \in \Bbb R^{|V| \times \cal Y}$是标签的集合。

在传统的机器学习分类任务中，我们需要学习一个$H$假设，使它可以把$X$映射到$Y$集合中。现在，我们可以利用嵌入到图$G$结构中的样本关系获得有意义的信息，进而获得更好的表现。

在文献中，这被称为关系分类。传统的方法把这个问题看作无向马尔可夫网络的推理问题，并且在给定网络结构的情况下，运用迭代近似推理算法取计算标签的后验分布。

我们提出一种不同的方法去获取网络的拓扑信息。而不是混合标签空间作为特征空间的一部分，我们提出一种无监督的方法可以得到具有捕捉网络结构能力的特征，并且它们与标签的分布是相关独立的。

我们的目标是学习$X_E \in \Bbb R^{|V| \times d}$，这里的$d$是潜在维数的一个小小的子集。这些低维表示是分布式的，意味着每种社交现象被这些维度的子集所表示，并且每个维度都贡献了空间所表达的社会概念的子集。

使用这些结构化的属性，我们就可以扩充特征空间，帮助进行分类决策。这些特征是通用的，可以用作任何分类算法（包括迭代算法）。因此，这些特征的最大好处就是易与简单的机器学习算法整合起来。



# 3. 学习社交表示

我们试图学习具有以下特征的社会表征:

+ **适应性** -- 真实的社交网络是不断进化的;新的社会关系不应该要求重新学习过程。
+ **社区感知** -- 潜在维度之间的距离应该代表一个度量标准，用于评估网络中相应成员之间的社会相似性。这使得具有同质性的网络可以泛化。
+ **低维** -- 当标记数据不足时，低维模型能更好地推广，并加速收敛和推理。
+ **连续** -- 我们需要潜在的表现来在连续空间中模拟部分社区成员。除了提供社区成员的细致视图外，一个连续的表示在社区之间有平滑的决策边界，这允许更健壮的分类。

我们的方法通过学习短随机游动流中的顶点表示法，使用最初为语言建模设计的优化技术来满足这些要求。在这里，我们回顾了随机游动和语言建模的基础知识，并描述了它们的组合如何满足我们的需求。

## 3.1 随机游走

我们定义随机游走的根节点$v_i$为$\cal W_{v_i}$。它是一个由$\cal W^1_{v_i},\cal W^2_{v_i},...,\cal W^k_{v_i}$组成的随机过程，$\cal W^{k+1}_{v_i}$是被随机选出的节点$v_k$的邻居。随机游走作为一种相似度度量的方式应用于内容推荐和社区发现。正是这种与本地结构的连接促使我们使用短随机游动流作为从网络中提取信息的基本工具。使用随机游走不仅可以获取社区信息，还有两个理想特性。**首先**，局部探索很容易并行化。几个随机的行人(在不同的线程、进程或机器中)可以同时探索同一图的不同部分。**其次**，依靠短随机游动获得的信息，可以适应图结构的微小变化，而不需要全局重新计算。我们可以用新的随机游动来迭代地更新所学习的模型，这对比更新整个图来说是次线性的。

## 3.2 连接：幂律

![这里写图片描述](figure2.png)

在选择在线随机游走作为我们主要捕捉图结构的方法之后，我们现在需要一个合适的方法去捕捉这些信息。如果一个连接图的度分布服从幂律定律，我们观测得到节点在短随机游走出现的频率也会服从**幂律分布**

我们工作的核心贡献是提出，过去用于自然语言建模的方法（符号频率服从幂律分布）也可以用于网络的社区结构建模。接下来将介绍自然语言建模，然后将其转化为学习节点表示，并使之满足我们的标准。

## 3.3 语言建模

语言建模的目标是估计一串特殊的词出现在全集的可能性。正式地说，给定一串词$W^n_1=(w_0,w_1,...,w_n)$，其中$w_i \in \cal V$($\cal V$是词汇表)，我们要在所有训练的集合中求出$Pr(w_n|w_0,w_1,...,w_{n-1})$的最大值。最近关于表示学习的工作聚焦在使用概率神经网络去建立通用的词汇表示，这超出了语言建模的原始目标的范围。

在这项工作中，我们通过短随机游走探索图，这展示了一种语言建模的一般化。这种直接的类比是在给定随机游走之前访问过的节点情况下，估计下一次要访问节点$v_i$的可能性。
$$
Pr(v_i|(v_1,v_2,...,v_{i-1})) \tag{1}
$$
我们的目标是学习一种潜在的表示，而不是一个节点再现的概率分布，因此我们引入一个映射函数$\Phi:v \in V \to \Bbb R^{|V| \times d}$。这个映射函数$\Phi$表示图中每个节点$V$之间潜在的社交表示。然后这个问题就变成估计以下可能性：
$$
Pr(v_i|(\Phi(v_1),\Phi(v_2),...,\Phi(v_{i-1}))) \tag{2}
$$
然而，随着游走的距离增大，计算这个目标函数变得不是那么容易。

一个在语言建模的最新的放松(relaxation)考虑到了这个预测问题。首先，不是用上下文去预测缺失的单词，而是用单词去预测它的上下文。其次，这里的上下文同时包括该单词的右边的词汇和左边的词汇。最后，它去除了这个问题的顺序约束。取而代之的是，这个模型需要最大化上下文出现的各个单词的概率，而无需知道其偏离给定单词的知识。在节点表示建模方面，这产生了如下优化问题：
$$
\underset{\Phi}{minimize}-logPr(\lbrace v_{i-w},...,v_{i-1},v_{i+1},...,v_{i+w} \rbrace / v_i | \Phi(v_i)) \tag{3}
$$
我们发现这些放松在社交表示学习上尤其可取。首先，顺序独立假设很好地获取了随机游走所提供的“接近”。另外，这个放松可以在某个时间给出一个节点的情况下，通过构建更小的模型加速训练时间。

解决上面式子的优化问题构建了局部图结构的节点之间的共享相似度表示。具有相似邻居的节点会获得相似的表示，可以在机器学习任务上一般化。

通过结合缩短的随机游走和神经语言模型我们建立一种可以满足我们所有期望特性的方法。这种方法生成了社交网络的低维表示，并且在向量空间连续。它表示了社区成员的潜在形式，并且由于这种方法输出有用的中间表示，它可以适应变化的网络拓扑。



# 4. 方法

在本节中，我们将讨论算法的主要组成部分。我们也提出了几种不同的方法，并讨论了它们的优点。

## 4.1 概况

在其他所有语言建模算法中，需要的输入仅为一个全集和一个词汇表$\cal V$。DeepWalk把随机游走作为自己的全集，图的节点作为自己的词汇表$(\cal V$ = $V)$。然而，最好在训练之前知道随机游走的$V$和节点的频率分布，不过这不是必须要的。

## 4.2 算法： DeepWalk

算法主要包括两个主要成分；第一是一个随机游走的生成器，第二是更新程序。**随机游走生成器**把图$G$作为输入，随机挑选一个节点$v_i$作为随机游走$\cal W_{v_i}$的根节点。每一步需要从上一个节点的邻居节点中随机挑选一个作为当前节点，直到达到最大长度$t$。在实验中我们把这个长度固定，但是并没有规定$t$必须取某个相同的值。这些游走可能重新回到起点，但是我们之前的结果并没有显示重新开始的任何优势。在实践过程中，我们在每个节点进行了$\gamma$次长度为$t$的随机游走。

![这里写图片描述](alg1.png)

算法1中的3-9行显示了我们方法的核心。外循环指定次数，$\gamma$，我们应该在哪个点开始随机游走。 我们认为每次迭代都是对数据进行一次“传递”，并在此传递过程中对每个节点进行一次抽样。在每次遍历的开始，我们都会生成一个随机的遍历顶点的顺序。这不是严格要求，但众所周知可以加快收敛的随机梯度下降。  

在内部循环中，我们遍历图上的所有顶点。对于每个顶点$v_i$，我们生成一个随机游走$|\cal W_{v_i}| = t$，然后用它来更新我们的表示。我们根据目标函数，使用**SkipGram**算法进行表示的更新。

### 4.2.1 SkipGram

![这里写图片描述](alg2.png)

SkipGram是一种语言模型，它使出现在窗口$w$中的单词在句子中的共现概率最大化。它使用如下独立假设近似方程3中的条件概率
$$
Pr\left(\{v_{i-w}, ..., v_{i+w}\}/v_i | \Phi(v_i) \right) = \prod_{j=i-w \& j\neq i}^{i+w}Pr(v_j | \Phi(v_i)) \tag{4}
$$
算法2 遍历出现在窗口$w$中的所有可能的随机配置遍历（第1-2行）中的dom步行。对于

每一个顶点，我们将每个顶点$v_j$映射到其当前表示向量$\Phi(v_j) \in \Bbb R^d$（见图3b）。给定$v_j$的表示，我们想要最大化它的邻居在这条线上的概率(第3行)。我们可以用几种分类器来学习这种后验分布。例如，使用逻辑回归对前面的问题进行建模，将产生大量的标签(即$|V|$)，其数量可能是数百万或数十亿。这些模型需要大量的计算资源，这些资源可以跨越整个计算机集群[4]。为了避免这种必要性，加快训练时间，我们使用层次结构来近似概率分布。

![这里写图片描述](figure3.png)

### 4.2.2 分层SoftMax

给定$u_k \in V$,计算$Pr(u_k|\Phi(v_j))$不是可取的。计算分区函数（归一化因子）是代价高的。如果我们把顶点分配给二叉树的叶节点，将预测问题转化为最大化层次结构中特定路径的概率(参见图3c)。如果到顶点$u_k$的路径由一系列树节点$(b_0, b_1, ..., b_{[log|V|]})$，$(b_0=root, b_{[log|V|]}=u_k)$，那么：
$$
Pr(u_k | \Phi(v_j)) = \prod_{l=1}^{[log|V|]}Pr(b_l | \Phi(v_j)) \tag{5}
$$
现在，$Pr(b_l | \Phi(v_j))$可由分配给节点$b_l$父节点的二进制分类器建模，如式6所示，
$$
Pr(b_l | \Phi(v_j)) = 1 / (1+e^{-\Phi(v_j).\Psi(b_l)}) \tag{6}
$$
其中$\Psi(b_l) \in \Bbb R^d$是分配给树节点$b_l$的父节点的表示形式。这减少了计算$Pr(u_k | \Phi(v_j)) $的复杂性，复杂度从$O(|V|)$降低到$O(log|V|)$。

通过为随机游动中频繁出现的顶点分配较短的路径，可以进一步加快训练过程。人工编码是为了减少树中频繁元素的访问时间。

### 4.2.3 最优化

这个模型的参数集合是$\theta = \lbrace \Phi,T \rbrace$，其大小都为$O(d|V|)$。随机梯度下降被用来优化这些参数。微分用后向传播算经进行计算。学习率$\alpha$初始化为2.5%，然后随着目前发现的节点的增加线性减小。

## 4.3 并行性

如图2所示，社交网络中随机游走的顶点的频率分布和语言中的单词都遵循幂律。这就导致了一条罕见顶点的长尾，因此，影响$\Phi$的更新在本质上是稀疏的。在有多个worker的情况下允许我们使用异步版本的随机梯度下降（ASGD）。鉴于我们的更新是稀疏的，并且我们没有获得访问模型共享参数的锁，因此ASGD将实现最佳收敛速度[36]。当我们在一台使用多线程的机器上运行实验时，已证明该技术具有高度的可扩展性，并且可以用于超大规模的机器学习[9]。图4展示了并行化DeepWalk的效果。它表明，随着我们将worker数量增加到8个，处理BlogCatalog和Flickr网络的速度是一致的（图4a）。它还表明，相对于串行运行DeepWalk而言，不会降低预测性能（图4b）。
![这里写图片描述](figure4.png)

## 4.4 算法变体

在这里，我们将讨论我们所提议的方法的一些变体，我们相信这些变体可能会很有趣。

### 4.4.1 Streaming

这种方法的一个有趣的变体是流式处理处理，哪些可以在不了解整个图的情况下实现。在这种变体中，图的小遍历被直接传递给表示学习代码，并直接更新模型。 对学习过程进行一些修改也是必要的。 首先，使用衰减的学习率可能不再是可取的，因为它假定了总语料库大小的知识。反而，我们可以将学习率$\alpha$初始化为一个小的常数值。这将需要更长的学习时间，但在某些应用程序中值得。其次，我们不一定要建立任何参数树。 如果$V$的基数已知（或可以有界），我们可以为该最大值构建Hierarchical Softmax树。 可以将顶点分配给其余叶子之一
当他们第一次见到。 如果我们有能力预先估计顶点频率，我们还可以使用人工编码(Huffman coding)来减少频繁的元素访问时间。

### 4.4.2 非随机游走

有些图是代理与一系列元素交互的副产品(例如，用户在网站上的页面导航)。当这样一个非随机游动流创建一个图时，我们可以使用这个过程来直接支持建模阶段。以这种方式采样的图不仅可以捕获与网络结构相关的信息，还可以捕获路径遍历的频率。

在我们看来，这种变体还包括语言建模。句子可以被看作是经过适当设计的语言网络的有目的的散步，而像SkipGram这样的语言模型就是为了捕捉这种行为而设计的。

这种方法可以与流变体相结合(第4.4.1节)在一个不断演化的网络中训练特征，而不需要显式地构造整个图。使用这种技术维护表示可以实现web级别的分类，而不需要处理web级别的图。

![这里写图片描述](table1.png)

