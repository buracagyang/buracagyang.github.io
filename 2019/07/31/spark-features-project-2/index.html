<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>【Spark】特征工程2-Transformers - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Spark MLlib中关于特征处理的相关算法，大致分为以下几组：  提取(Extraction)：从“原始”数据中提取特征 转换(Transformation)：缩放，转换或修改特征 选择(Selection)：从较大的一组特征中选择一个子集 局部敏感哈希(Locality Sensitive Hashing，LSH)：这类算法将特征变换的各个方面与其他算法相结合。  本文介绍第二组： 特征转换"><meta property="og:type" content="blog"><meta property="og:title" content="【Spark】特征工程2-Transformers"><meta property="og:url" content="http://example.com/2019/07/31/spark-features-project-2/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="Spark MLlib中关于特征处理的相关算法，大致分为以下几组：  提取(Extraction)：从“原始”数据中提取特征 转换(Transformation)：缩放，转换或修改特征 选择(Selection)：从较大的一组特征中选择一个子集 局部敏感哈希(Locality Sensitive Hashing，LSH)：这类算法将特征变换的各个方面与其他算法相结合。  本文介绍第二组： 特征转换"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2019-07-31T06:37:16.000Z"><meta property="article:modified_time" content="2019-08-09T02:32:28.000Z"><meta property="article:author" content="Buracag"><meta property="article:tag" content="技术备忘"><meta property="article:tag" content="大数据"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2019/07/31/spark-features-project-2/"},"headline":"【Spark】特征工程2-Transformers","image":["http://example.com/img/og_image.png"],"datePublished":"2019-07-31T06:37:16.000Z","dateModified":"2019-08-09T02:32:28.000Z","author":{"@type":"Person","name":"Buracag"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"Spark MLlib中关于特征处理的相关算法，大致分为以下几组：  提取(Extraction)：从“原始”数据中提取特征 转换(Transformation)：缩放，转换或修改特征 选择(Selection)：从较大的一组特征中选择一个子集 局部敏感哈希(Locality Sensitive Hashing，LSH)：这类算法将特征变换的各个方面与其他算法相结合。  本文介绍第二组： 特征转换"}</script><link rel="canonical" href="http://example.com/2019/07/31/spark-features-project-2/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-31T06:37:16.000Z" title="2019/7/31 14:37:16">2019-07-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-09T02:32:28.000Z" title="2019/8/9 10:32:28">2019-08-09</time></span><span class="level-item">an hour read (About 7011 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">【Spark】特征工程2-Transformers</h1><div class="content"><p>Spark MLlib中关于特征处理的相关算法，大致分为以下几组：</p>
<ul>
<li>提取(Extraction)：从“原始”数据中提取特征</li>
<li>转换(Transformation)：缩放，转换或修改特征</li>
<li>选择(Selection)：从较大的一组特征中选择一个子集</li>
<li>局部敏感哈希(Locality Sensitive Hashing，LSH)：这类算法将特征变换的各个方面与其他算法相结合。</li>
</ul>
<p>本文介绍第二组： 特征转换器(Transformers)</p>
<span id="more"></span>
<h1 id="1-特征转换器"><a href="#1-特征转换器" class="headerlink" title="1. 特征转换器"></a>1. 特征转换器</h1><h2 id="1-1-分词器-Tokenizer"><a href="#1-1-分词器-Tokenizer" class="headerlink" title="1.1 分词器(Tokenizer)"></a>1.1 分词器(Tokenizer)</h2><p>标记化(Tokenization)是将文本（例如句子）分解为单个术语（通常是单词）的过程。 一个简单的Tokenizer类提供此功能。 下面的示例显示了如何将句子拆分为单词序列。</p>
<p>RegexTokenizer允许基于正则表达式（正则表达式）匹配的更高级标记化。 默认情况下，参数“pattern”（正则表达式，默认值：“\\s +”）用作分隔输入文本的分隔符。 或者，用户可以将参数“gap”设置为false，指示正则表达式“pattern”表示“令牌”而不是分割间隙，并找到所有匹配的出现作为标记化结果。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 14:58</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : tokenizer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Tokenizer, RegexTokenizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col, udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;TokenizerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    sentenceDataFrame = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, <span class="string">&quot;Hi I heard about Spark&quot;</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="string">&quot;Logistic,regression,models,are,neat&quot;</span>)</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;sentence&quot;</span>])</span><br><span class="line"></span><br><span class="line">    tokenizer = Tokenizer(inputCol=<span class="string">&quot;sentence&quot;</span>, outputCol=<span class="string">&quot;words&quot;</span>)</span><br><span class="line"></span><br><span class="line">    regexTokenizer = RegexTokenizer(inputCol=<span class="string">&quot;sentence&quot;</span>, outputCol=<span class="string">&quot;words&quot;</span>, pattern=<span class="string">&quot;\\W&quot;</span>)</span><br><span class="line">    <span class="comment"># 也可以选择， pattern=&quot;\\w+&quot;, gaps(False)</span></span><br><span class="line"></span><br><span class="line">    countTokens = udf(<span class="keyword">lambda</span> words: <span class="built_in">len</span>(words), IntegerType())</span><br><span class="line"></span><br><span class="line">    tokenized = tokenizer.transform(sentenceDataFrame)</span><br><span class="line">    tokenized.select(<span class="string">&quot;sentence&quot;</span>, <span class="string">&quot;words&quot;</span>).withColumn(<span class="string">&quot;tokens&quot;</span>, countTokens(col(<span class="string">&quot;words&quot;</span>))).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    regexTokenized = regexTokenizer.transform(sentenceDataFrame)</span><br><span class="line">    regexTokenized.select(<span class="string">&quot;sentence&quot;</span>, <span class="string">&quot;words&quot;</span>).withColumn(<span class="string">&quot;tokens&quot;</span>, countTokens(col(<span class="string">&quot;words&quot;</span>))).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|sentence                           |words                                     |tokens|</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |</span><br><span class="line">|I wish Java could use <span class="keyword">case</span> classes |[i, wish, java, could, use, <span class="keyword">case</span>, classes]|7     |</span><br><span class="line">|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|sentence                           |words                                     |tokens|</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |</span><br><span class="line">|I wish Java could use <span class="keyword">case</span> classes |[i, wish, java, could, use, <span class="keyword">case</span>, classes]|7     |</span><br><span class="line">|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br></pre></td></tr></table></figure>
<h2 id="1-2-StopWordsRemover"><a href="#1-2-StopWordsRemover" class="headerlink" title="1.2 StopWordsRemover"></a>1.2 StopWordsRemover</h2><p>停用词是应该从输入中排除的词，通常是因为词经常出现而且没有那么多含义。</p>
<p>StopWordsRemover将字符串序列（例如，Tokenizer的输出）作为输入，并从输入序列中删除所有停用词。 停用词列表由stopWords参数指定。 通过调用StopWordsRemover.loadDefaultStopWords（语言）可以访问某些语言的默认停用词，其中可用选项为“danish”，“dutch”，“english”，“finnish”，“french”，“german”，“hungarian”，italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”。 布尔参数caseSensitive指示匹配项是否区分大小写（默认为false）。</p>
<p><strong>举例</strong></p>
<p>假设我们有以下具有列id和raw的DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">id</span> | raw</span><br><span class="line">----|----------</span><br><span class="line"> 0  | [I, saw, the, red, baloon]</span><br><span class="line"> 1  | [Mary, had, a, little, lamb]</span><br></pre></td></tr></table></figure>
<p>经过停用词处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 17:26</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : stopwords_remover_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StopWordsRemover</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;StopWordsRemoverExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    sentenceData = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, [<span class="string">&quot;I&quot;</span>, <span class="string">&quot;saw&quot;</span>, <span class="string">&quot;the&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;balloon&quot;</span>]),</span><br><span class="line">        (<span class="number">1</span>, [<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;had&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;little&quot;</span>, <span class="string">&quot;lamb&quot;</span>])</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;raw&quot;</span>])</span><br><span class="line"></span><br><span class="line">    remover = StopWordsRemover(inputCol=<span class="string">&quot;raw&quot;</span>, outputCol=<span class="string">&quot;filtered&quot;</span>)</span><br><span class="line">    remover.transform(sentenceData).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+---+----------------------------+--------------------+</span><br><span class="line">|<span class="built_in">id</span> |raw                         |filtered            |</span><br><span class="line">+---+----------------------------+--------------------+</span><br><span class="line">|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |</span><br><span class="line">|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|</span><br><span class="line">+---+----------------------------+--------------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-3-n-gram"><a href="#1-3-n-gram" class="headerlink" title="1.3 n-gram"></a>1.3 n-gram</h2><p>对于某些整数n，n-gram是n个tokens（通常是单词）的序列。 NGram类可用于将输入要素转换为n-gram。</p>
<p>NGram将字符串序列（例如，Tokenizer的输出）作为输入。 参数n用于确定每个n-gram中的项数。 输出将由一系列n-gram组成，其中每个n-gram由n个连续单词的空格分隔的字符串表示。 如果输入序列包含少于n个字符串，则不会生成输出。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 17:32</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : n_gram_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> NGram</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;NGramExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    wordDataFrame = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, [<span class="string">&quot;Hi&quot;</span>, <span class="string">&quot;I&quot;</span>, <span class="string">&quot;heard&quot;</span>, <span class="string">&quot;about&quot;</span>, <span class="string">&quot;Spark&quot;</span>]),</span><br><span class="line">        (<span class="number">1</span>, [<span class="string">&quot;I&quot;</span>, <span class="string">&quot;wish&quot;</span>, <span class="string">&quot;Java&quot;</span>, <span class="string">&quot;could&quot;</span>, <span class="string">&quot;use&quot;</span>, <span class="string">&quot;case&quot;</span>, <span class="string">&quot;classes&quot;</span>]),</span><br><span class="line">        (<span class="number">2</span>, [<span class="string">&quot;Logistic&quot;</span>, <span class="string">&quot;regression&quot;</span>, <span class="string">&quot;models&quot;</span>, <span class="string">&quot;are&quot;</span>, <span class="string">&quot;neat&quot;</span>])</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;words&quot;</span>])</span><br><span class="line"></span><br><span class="line">    ngram = NGram(n=<span class="number">2</span>, inputCol=<span class="string">&quot;words&quot;</span>, outputCol=<span class="string">&quot;ngrams&quot;</span>)</span><br><span class="line"></span><br><span class="line">    ngramDataFrame = ngram.transform(wordDataFrame)</span><br><span class="line">    ngramDataFrame.select(<span class="string">&quot;ngrams&quot;</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+------------------------------------------------------------------+</span><br><span class="line">|ngrams                                                            |</span><br><span class="line">+------------------------------------------------------------------+</span><br><span class="line">|[Hi I, I heard, heard about, about Spark]                         |</span><br><span class="line">|[I wish, wish Java, Java could, could use, use <span class="keyword">case</span>, <span class="keyword">case</span> classes]|</span><br><span class="line">|[Logistic regression, regression models, models are, are neat]    |</span><br><span class="line">+------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-4-二元化-Binarizer"><a href="#1-4-二元化-Binarizer" class="headerlink" title="1.4 二元化(Binarizer)"></a>1.4 二元化(Binarizer)</h2><p>二元化是将数值特征阈值化为二元（0/1）特征的过程。</p>
<p>Binarizer采用公共参数inputCol和outputCol，以及二元化的阈值。 大于阈值的特征值被二进制化为1.0; 小于等于阈值的值被二值化为0.0。 inputCol支持Vector和Double类型。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 17:36</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : binarizer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Binarizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;BinarizerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    continuousDataFrame = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, <span class="number">0.1</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">0.8</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">0.2</span>)</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;feature&quot;</span>])</span><br><span class="line"></span><br><span class="line">    binarizer = Binarizer(threshold=<span class="number">0.5</span>, inputCol=<span class="string">&quot;feature&quot;</span>, outputCol=<span class="string">&quot;binarized_feature&quot;</span>)</span><br><span class="line">    binarizedDataFrame = binarizer.transform(continuousDataFrame)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Binarizer output with Threshold = %f&quot;</span> % binarizer.getThreshold())</span><br><span class="line">    binarizedDataFrame.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Binarizer output with Threshold = 0.500000</span><br><span class="line">+---+-------+-----------------+</span><br><span class="line">| <span class="built_in">id</span>|feature|binarized_feature|</span><br><span class="line">+---+-------+-----------------+</span><br><span class="line">|  0|    0.1|              0.0|</span><br><span class="line">|  1|    0.8|              1.0|</span><br><span class="line">|  2|    0.2|              0.0|</span><br><span class="line">+---+-------+-----------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-5-主成分分析-PCA"><a href="#1-5-主成分分析-PCA" class="headerlink" title="1.5 主成分分析(PCA)"></a>1.5 主成分分析(PCA)</h2><p>PCA是一种统计过程，它使用正交变换将可能相关变量的一组观察值转换为称为主成分的线性不相关变量的一组值。 PCA类使用PCA训练模型以将向量映射到低维空间。 下面的示例显示了如何将5维特征向量映射到3维主成分中。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 17:39</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : pca_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;PCAExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    data = [(Vectors.sparse(<span class="number">5</span>, [(<span class="number">1</span>, <span class="number">1.0</span>), (<span class="number">3</span>, <span class="number">7.0</span>)]),),</span><br><span class="line">            (Vectors.dense([<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>]),),</span><br><span class="line">            (Vectors.dense([<span class="number">4.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>]),)]</span><br><span class="line">    df = spark.createDataFrame(data, [<span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    pca = PCA(k=<span class="number">3</span>, inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;pcaFeatures&quot;</span>)</span><br><span class="line">    model = pca.fit(df)</span><br><span class="line"></span><br><span class="line">    result = model.transform(df).select(<span class="string">&quot;pcaFeatures&quot;</span>)</span><br><span class="line">    result.show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------------------------------+</span><br><span class="line">|pcaFeatures                                                |</span><br><span class="line">+-----------------------------------------------------------+</span><br><span class="line">|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |</span><br><span class="line">|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|</span><br><span class="line">|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |</span><br><span class="line">+-----------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-6-多项式扩展-PolynomialExpansion"><a href="#1-6-多项式扩展-PolynomialExpansion" class="headerlink" title="1.6 多项式扩展(PolynomialExpansion)"></a>1.6 多项式扩展(PolynomialExpansion)</h2><p>多项式展开是将要素扩展为多项式空间的过程，该多项式空间由原始维度的n度组合制定。 PolynomialExpansion类提供此功能。 以下示例显示如何将特征扩展为3度多项式空间。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 17:44</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : polynomial_expansion_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> PolynomialExpansion</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;PolynomialExpansionExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame([</span><br><span class="line">        (Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">        (Vectors.dense([<span class="number">0.0</span>, <span class="number">0.0</span>]),),</span><br><span class="line">        (Vectors.dense([<span class="number">3.0</span>, -<span class="number">1.0</span>]),)</span><br><span class="line">    ], [<span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    polyExpansion = PolynomialExpansion(degree=<span class="number">3</span>, inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;polyFeatures&quot;</span>)</span><br><span class="line">    polyDF = polyExpansion.transform(df)</span><br><span class="line"></span><br><span class="line">    polyDF.show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----------+------------------------------------------+</span><br><span class="line">|features  |polyFeatures                              |</span><br><span class="line">+----------+------------------------------------------+</span><br><span class="line">|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |</span><br><span class="line">|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |</span><br><span class="line">|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|</span><br><span class="line">+----------+------------------------------------------+</span><br></pre></td></tr></table></figure>
<p>简单解释一下：</p>
<p>原始特征: $x_1$, $x_2$</p>
<p>二阶多项式的展开部分：$x_1^2$, $x_1x_2$, $x_2^2$</p>
<p>三阶多项式的展开部分：$x_1^2x_2$, $x_1x_2^2$, $x_1^3$, $x_2^3$</p>
<p>所以得到,</p>
<p>二阶多项式扩展为： 原始特征 + 二阶多项式的展开部分</p>
<p>三阶多项式扩展为： 原始特征 + 二阶多项式的展开部分 + 三阶多项式的展开部分</p>
<h2 id="1-7-离散余弦距离-Discrete-Cosine-Transform-DCT"><a href="#1-7-离散余弦距离-Discrete-Cosine-Transform-DCT" class="headerlink" title="1.7 离散余弦距离(Discrete Cosine Transform, DCT)"></a>1.7 离散余弦距离(Discrete Cosine Transform, DCT)</h2><p>离散余弦变换将时域中的长度N实值序列变换为频域中的另一长度N实值序列。 DCT类提供此功能，实现DCT-II并将结果缩放$\frac{1}{\sqrt{2}}$，使得变换的表示矩阵是单一的。应用于变换序列没有移位（例如，变换序列的第0个元素是第0个DCT系数而不是N / 2个）。</p>
<p> <strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 17:59</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : dct_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> DCT</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;DCTExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame([</span><br><span class="line">        (Vectors.dense([<span class="number">0.0</span>, <span class="number">1.0</span>, -<span class="number">2.0</span>, <span class="number">3.0</span>]),),</span><br><span class="line">        (Vectors.dense([-<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>, -<span class="number">7.0</span>]),),</span><br><span class="line">        (Vectors.dense([<span class="number">14.0</span>, -<span class="number">2.0</span>, -<span class="number">5.0</span>, <span class="number">1.0</span>]),)], [<span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    dct = DCT(inverse=<span class="literal">False</span>, inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;featuresDCT&quot;</span>)</span><br><span class="line">    dctDf = dct.transform(df)</span><br><span class="line"></span><br><span class="line">    dctDf.select(<span class="string">&quot;featuresDCT&quot;</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------------------------------------------+</span><br><span class="line">|featuresDCT                                                     |</span><br><span class="line">+----------------------------------------------------------------+</span><br><span class="line">|[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]|</span><br><span class="line">|[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677]  |</span><br><span class="line">|[4.0,9.304453421915744,11.000000000000002,1.5579302036357163]   |</span><br><span class="line">+----------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-8-字符串索引器-StringIndexer"><a href="#1-8-字符串索引器-StringIndexer" class="headerlink" title="1.8 字符串索引器(StringIndexer)"></a>1.8 字符串索引器(StringIndexer)</h2><p>StringIndexer将标签的字符串列编码为标签索引列。 索引在[0，numLabels)中，按标签频率排序，因此最常见的标签得到索引0。如果用户选择保留它们，则看不见的标签将被放在索引numLabels处。 如果输入列是数字，我们将其转换为字符串并索引字符串值。 当下游管道组件（如Estimator或Transformer）使用此字符串索引标签时，必须将组件的输入列设置为此字符串索引列名称。 在许多情况下，您可以使用setInputCol设置输入列。</p>
<p><strong>举例</strong></p>
<p>假设我们有以下DataFrame，列id和类别：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">id</span> | category</span><br><span class="line">----|----------</span><br><span class="line"> 0  | a</span><br><span class="line"> 1  | b</span><br><span class="line"> 2  | c</span><br><span class="line"> 3  | a</span><br><span class="line"> 4  | a</span><br><span class="line"> 5  | c</span><br></pre></td></tr></table></figure>
<p>category是一个包含三个标签的字符串列：“a”，“b”和“c”。 使用StringIndexer作为输入列，categoryIndex作为输出列，我们应该得到以下结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> | category | categoryIndex</span><br><span class="line">----|----------|---------------</span><br><span class="line"> 0  | a        | 0.0</span><br><span class="line"> 1  | b        | 2.0</span><br><span class="line"> 2  | c        | 1.0</span><br><span class="line"> 3  | a        | 0.0</span><br><span class="line"> 4  | a        | 0.0</span><br><span class="line"> 5  | c        | 1.0</span><br></pre></td></tr></table></figure>
<p>“a”得到索引0，因为它是最常见的，其次是索引1的“c”和索引2的“b”。</p>
<p>此外，当您在一个数据集上使用StringIndexer然后使用它来转换另一个数据集时，有三种策略可以解决StringIndexer如何处理看不见的标签：</p>
<ul>
<li>抛出异常（这是默认值）</li>
<li>完全跳过包含看不见的标签的行, “skip”</li>
<li>将看不见的标签放在索引numLabels的特殊附加存储桶中, “keep”</li>
</ul>
<p><strong>举例</strong></p>
<p>让我们回到之前的示例，但这次重用我们之前在以下数据集上定义的StringIndexer：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> | category</span><br><span class="line">----|----------</span><br><span class="line"> 0  | a</span><br><span class="line"> 1  | b</span><br><span class="line"> 2  | c</span><br><span class="line"> 3  | d</span><br><span class="line"> 4  | e</span><br></pre></td></tr></table></figure>
<p>如果您没有设置StringIndexer如何处理看不见的标签或将其设置为“error”，则会抛出异常。 但是，如果您调用了setHandleInvalid<strong>（“skip”）</strong>，则将生成以下数据集：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> | category | categoryIndex</span><br><span class="line">----|----------|---------------</span><br><span class="line"> 0  | a        | 0.0</span><br><span class="line"> 1  | b        | 2.0</span><br><span class="line"> 2  | c        | 1.0</span><br></pre></td></tr></table></figure>
<p>请注意，不显示包含“d”或“e”的行。如果调用setHandleInvalid<strong>（“keep”）</strong>，将生成以下数据集：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> | category | categoryIndex</span><br><span class="line">----|----------|---------------</span><br><span class="line"> 0  | a        | 0.0</span><br><span class="line"> 1  | b        | 2.0</span><br><span class="line"> 2  | c        | 1.0</span><br><span class="line"> 3  | d        | 3.0</span><br><span class="line"> 4  | e        | 3.0</span><br></pre></td></tr></table></figure>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 18:11</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : string_indexer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;StringIndexerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame(</span><br><span class="line">        [(<span class="number">0</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">4</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">5</span>, <span class="string">&quot;c&quot;</span>)],</span><br><span class="line">        [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;category&quot;</span>])</span><br><span class="line"></span><br><span class="line">    indexer = StringIndexer(inputCol=<span class="string">&quot;category&quot;</span>, outputCol=<span class="string">&quot;categoryIndex&quot;</span>)</span><br><span class="line">    indexed = indexer.fit(df).transform(df)</span><br><span class="line">    indexed.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="1-9-IndexToString"><a href="#1-9-IndexToString" class="headerlink" title="1.9 IndexToString"></a>1.9 IndexToString</h2><p>与StringIndexer相反，IndexToString将一列标签索引映射回包含原始标签作为字符串的列。 一个常见的用例是使用StringIndexer从标签生成索引，使用这些索引训练模型，并使用IndexToString从预测索引列中检索原始标签。 但是，您可以自由提供自己的标签。</p>
<p><strong>举例</strong></p>
<p>将categoryIndex作为输入列应用IndexToString，将originalCategory作为输出列，我们可以检索原始标签（它们将从列的元数据中推断出来）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 18:59</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : index_to_string_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> IndexToString, StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;IndexToStringExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame(</span><br><span class="line">        [(<span class="number">0</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">4</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">5</span>, <span class="string">&quot;c&quot;</span>)],</span><br><span class="line">        [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;category&quot;</span>])</span><br><span class="line"></span><br><span class="line">    indexer = StringIndexer(inputCol=<span class="string">&quot;category&quot;</span>, outputCol=<span class="string">&quot;categoryIndex&quot;</span>)</span><br><span class="line">    model = indexer.fit(df)</span><br><span class="line">    indexed = model.transform(df)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Transformed string column &#x27;%s&#x27; to indexed column &#x27;%s&#x27;&quot;</span> % (indexer.getInputCol(), indexer.getOutputCol()))</span><br><span class="line">    indexed.show()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;StringIndexer will store labels in output column metadata\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    converter = IndexToString(inputCol=<span class="string">&quot;categoryIndex&quot;</span>, outputCol=<span class="string">&quot;originalCategory&quot;</span>)</span><br><span class="line">    converted = converter.transform(indexed)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Transformed indexed column &#x27;%s&#x27; back to original string column &#x27;%s&#x27; using &quot;</span></span><br><span class="line">          <span class="string">&quot;labels in metadata&quot;</span> % (converter.getInputCol(), converter.getOutputCol()))</span><br><span class="line">    converted.select(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;categoryIndex&quot;</span>, <span class="string">&quot;originalCategory&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Transformed string column <span class="string">&#x27;category&#x27;</span> to indexed column <span class="string">&#x27;categoryIndex&#x27;</span></span><br><span class="line">+---+--------+-------------+</span><br><span class="line">| <span class="built_in">id</span>|category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|  0|       a|          0.0|</span><br><span class="line">|  1|       b|          2.0|</span><br><span class="line">|  2|       c|          1.0|</span><br><span class="line">|  3|       a|          0.0|</span><br><span class="line">|  4|       a|          0.0|</span><br><span class="line">|  5|       c|          1.0|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line"></span><br><span class="line">StringIndexer will store labels <span class="keyword">in</span> output column metadata</span><br><span class="line"></span><br><span class="line">Transformed indexed column <span class="string">&#x27;categoryIndex&#x27;</span> back to original string column <span class="string">&#x27;originalCategory&#x27;</span> using labels <span class="keyword">in</span> metadata</span><br><span class="line">+---+-------------+----------------+</span><br><span class="line">| <span class="built_in">id</span>|categoryIndex|originalCategory|</span><br><span class="line">+---+-------------+----------------+</span><br><span class="line">|  0|          0.0|               a|</span><br><span class="line">|  1|          2.0|               b|</span><br><span class="line">|  2|          1.0|               c|</span><br><span class="line">|  3|          0.0|               a|</span><br><span class="line">|  4|          0.0|               a|</span><br><span class="line">|  5|          1.0|               c|</span><br><span class="line">+---+-------------+----------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-10-One-Hot-OneHotEncoderEstimator"><a href="#1-10-One-Hot-OneHotEncoderEstimator" class="headerlink" title="1.10 One-Hot(OneHotEncoderEstimator)"></a>1.10 One-Hot(OneHotEncoderEstimator)</h2><p>One-hot编码将表示为标签索引的分类特征映射到二进制向量，该二进制向量具有至多单个一个值，该值表示所有特征值集合中存在特定特征值。 此编码允许期望连续特征（例如Logistic回归）的算法使用分类特征。 对于字符串类型输入数据，通常首先使用StringIndexer对分类特征进行编码。</p>
<p>OneHotEncoderEstimator可以转换多个列，为每个输入列返回一个热编码的输出向量列。 通常使用VectorAssembler将这些向量合并为单个特征向量。</p>
<p>OneHotEncoderEstimator支持handleInvalid参数，以选择在转换数据期间如何处理无效输入。 可用选项包括’keep’（任何无效输入分配给额外的分类索引）和’error’（抛出错误）。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 19:05</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : onehot_encoder_estimator_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoderEstimator</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;OneHotEncoderEstimatorExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分类特征通常先用StringIndexer先进行编码</span></span><br><span class="line">    df = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        (<span class="number">1.0</span>, <span class="number">0.0</span>),</span><br><span class="line">        (<span class="number">2.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        (<span class="number">0.0</span>, <span class="number">2.0</span>),</span><br><span class="line">        (<span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        (<span class="number">2.0</span>, <span class="number">0.0</span>)</span><br><span class="line">    ], [<span class="string">&quot;categoryIndex1&quot;</span>, <span class="string">&quot;categoryIndex2&quot;</span>])</span><br><span class="line"></span><br><span class="line">    encoder = OneHotEncoderEstimator(inputCols=[<span class="string">&quot;categoryIndex1&quot;</span>, <span class="string">&quot;categoryIndex2&quot;</span>],</span><br><span class="line">                                     outputCols=[<span class="string">&quot;categoryVec1&quot;</span>, <span class="string">&quot;categoryVec2&quot;</span>])</span><br><span class="line">    model = encoder.fit(df)</span><br><span class="line">    encoded = model.transform(df)</span><br><span class="line">    encoded.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+--------------+--------------+-------------+-------------+</span><br><span class="line">|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|</span><br><span class="line">+--------------+--------------+-------------+-------------+</span><br><span class="line">|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|</span><br><span class="line">|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|</span><br><span class="line">|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|</span><br><span class="line">|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|</span><br><span class="line">|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|</span><br><span class="line">|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|</span><br><span class="line">+--------------+--------------+-------------+-------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-11-矢量索引器-VectorIndexer"><a href="#1-11-矢量索引器-VectorIndexer" class="headerlink" title="1.11 矢量索引器(VectorIndexer)"></a>1.11 矢量索引器(VectorIndexer)</h2><p>VectorIndexer帮助索引Vectors的数据集中的分类特征。它既可以自动决定哪些特征是分类的，也可以将原始值转换为类别索引。具体来说，它执行以下操作：</p>
<ol>
<li>获取Vector类型的输入列和参数maxCategories。</li>
<li>根据不同值的数量确定哪些要素应该是分类的，其中最多maxCategories的要素被声明为分类。为每个分类特征计算基于0的类别索引。</li>
<li>索引分类要素并将原始要素值转换为索引。</li>
<li>索引分类特征允许决策树和树集合等算法适当地处理分类特征，从而提高性能。</li>
</ol>
<p><strong>举例</strong></p>
<p>在下面的示例中，我们读入标记点的数据集，然后使用VectorIndexer确定哪些要素应被视为分类。我们将分类特征值转换为它们的索引。然后，可以将转换后的数据传递给处理分类特征的DecisionTreeRegressor等算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 19:11</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : vector_indexer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;VectorIndexerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    data = spark.read.<span class="built_in">format</span>(<span class="string">&quot;libsvm&quot;</span>).load(<span class="string">&quot;../data/mllib/sample_libsvm_data.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    indexer = VectorIndexer(inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;indexed&quot;</span>, maxCategories=<span class="number">10</span>)</span><br><span class="line">    indexerModel = indexer.fit(data)</span><br><span class="line"></span><br><span class="line">    categoricalFeatures = indexerModel.categoryMaps</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Chose %d categorical features: %s&quot;</span> %</span><br><span class="line">          (<span class="built_in">len</span>(categoricalFeatures), <span class="string">&quot;, &quot;</span>.join(<span class="built_in">str</span>(k) <span class="keyword">for</span> k <span class="keyword">in</span> categoricalFeatures.keys())))</span><br><span class="line"></span><br><span class="line">    indexedData = indexerModel.transform(data)</span><br><span class="line">    indexedData.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|label|            features|             indexed|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|  0.0|(692,[127,128,129...|(692,[127,128,129...|</span><br><span class="line">|  1.0|(692,[158,159,160...|(692,[158,159,160...|</span><br><span class="line">|  1.0|(692,[124,125,126...|(692,[124,125,126...|</span><br><span class="line">|  1.0|(692,[152,153,154...|(692,[152,153,154...|</span><br><span class="line">|  1.0|(692,[151,152,153...|(692,[151,152,153...|</span><br><span class="line">|  0.0|(692,[129,130,131...|(692,[129,130,131...|</span><br><span class="line">|  1.0|(692,[158,159,160...|(692,[158,159,160...|</span><br><span class="line">|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|</span><br><span class="line">|  0.0|(692,[154,155,156...|(692,[154,155,156...|</span><br><span class="line">|  0.0|(692,[127,128,129...|(692,[127,128,129...|</span><br><span class="line">|  1.0|(692,[154,155,156...|(692,[154,155,156...|</span><br><span class="line">|  0.0|(692,[153,154,155...|(692,[153,154,155...|</span><br><span class="line">|  0.0|(692,[151,152,153...|(692,[151,152,153...|</span><br><span class="line">|  1.0|(692,[129,130,131...|(692,[129,130,131...|</span><br><span class="line">|  0.0|(692,[154,155,156...|(692,[154,155,156...|</span><br><span class="line">|  1.0|(692,[150,151,152...|(692,[150,151,152...|</span><br><span class="line">|  0.0|(692,[124,125,126...|(692,[124,125,126...|</span><br><span class="line">|  0.0|(692,[152,153,154...|(692,[152,153,154...|</span><br><span class="line">|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|</span><br><span class="line">|  1.0|(692,[124,125,126...|(692,[124,125,126...|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure>
<h2 id="1-12-交互作用-Interaction"><a href="#1-12-交互作用-Interaction" class="headerlink" title="1.12 交互作用(Interaction)"></a>1.12 交互作用(Interaction)</h2><p>Interaction是一个Transformer，它接收向量或双值列，并生成一个向量列，其中包含每个输入列中一个值的所有组合的乘积。</p>
<p>例如，如果您有2个矢量类型列，每个列都有3个维度作为输入列，那么您将获得9维向量作为输出列。</p>
<p><strong>举例</strong></p>
<p>假设我们有以下DataFrame，其列为“id1”，“vec1”和“vec2”：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">id1|vec1          |vec2          </span><br><span class="line">---|--------------|--------------</span><br><span class="line">1  |[1.0,2.0,3.0] |[8.0,4.0,5.0] </span><br><span class="line">2  |[4.0,3.0,8.0] |[7.0,9.0,8.0] </span><br><span class="line">3  |[6.0,1.0,9.0] |[2.0,3.0,6.0] </span><br><span class="line">4  |[10.0,8.0,6.0]|[9.0,4.0,5.0] </span><br><span class="line">5  |[9.0,2.0,7.0] |[10.0,7.0,3.0]</span><br><span class="line">6  |[1.0,1.0,4.0] |[2.0,8.0,4.0] </span><br></pre></td></tr></table></figure>
<p>将交互应用于这些输入列，然后将interactedCol作为输出列包含:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">id1|vec1          |vec2          |interactedCol                                         </span><br><span class="line">---|--------------|--------------|------------------------------------------------------</span><br><span class="line"><span class="number">1</span>  |[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>] |[<span class="number">8.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>] |[<span class="number">8.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>,<span class="number">16.0</span>,<span class="number">8.0</span>,<span class="number">10.0</span>,<span class="number">24.0</span>,<span class="number">12.0</span>,<span class="number">15.0</span>]            </span><br><span class="line"><span class="number">2</span>  |[<span class="number">4.0</span>,<span class="number">3.0</span>,<span class="number">8.0</span>] |[<span class="number">7.0</span>,<span class="number">9.0</span>,<span class="number">8.0</span>] |[<span class="number">56.0</span>,<span class="number">72.0</span>,<span class="number">64.0</span>,<span class="number">42.0</span>,<span class="number">54.0</span>,<span class="number">48.0</span>,<span class="number">112.0</span>,<span class="number">144.0</span>,<span class="number">128.0</span>]     </span><br><span class="line"><span class="number">3</span>  |[<span class="number">6.0</span>,<span class="number">1.0</span>,<span class="number">9.0</span>] |[<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">6.0</span>] |[<span class="number">36.0</span>,<span class="number">54.0</span>,<span class="number">108.0</span>,<span class="number">6.0</span>,<span class="number">9.0</span>,<span class="number">18.0</span>,<span class="number">54.0</span>,<span class="number">81.0</span>,<span class="number">162.0</span>]        </span><br><span class="line"><span class="number">4</span>  |[<span class="number">10.0</span>,<span class="number">8.0</span>,<span class="number">6.0</span>]|[<span class="number">9.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>] |[<span class="number">360.0</span>,<span class="number">160.0</span>,<span class="number">200.0</span>,<span class="number">288.0</span>,<span class="number">128.0</span>,<span class="number">160.0</span>,<span class="number">216.0</span>,<span class="number">96.0</span>,<span class="number">120.0</span>]</span><br><span class="line"><span class="number">5</span>  |[<span class="number">9.0</span>,<span class="number">2.0</span>,<span class="number">7.0</span>] |[<span class="number">10.0</span>,<span class="number">7.0</span>,<span class="number">3.0</span>]|[<span class="number">450.0</span>,<span class="number">315.0</span>,<span class="number">135.0</span>,<span class="number">100.0</span>,<span class="number">70.0</span>,<span class="number">30.0</span>,<span class="number">350.0</span>,<span class="number">245.0</span>,<span class="number">105.0</span>] </span><br><span class="line"><span class="number">6</span>  |[<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">4.0</span>] |[<span class="number">2.0</span>,<span class="number">8.0</span>,<span class="number">4.0</span>] |[<span class="number">12.0</span>,<span class="number">48.0</span>,<span class="number">24.0</span>,<span class="number">12.0</span>,<span class="number">48.0</span>,<span class="number">24.0</span>,<span class="number">48.0</span>,<span class="number">192.0</span>,<span class="number">96.0</span>] </span><br></pre></td></tr></table></figure>
<p>示例脚本(Java/Scala)请参照<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.3.2/ml-features.html#feature-transformers">这里</a>。</p>
<h2 id="1-13-标准化-Normalizer"><a href="#1-13-标准化-Normalizer" class="headerlink" title="1.13 标准化(Normalizer)"></a>1.13 标准化(Normalizer)</h2><p>Normalizer是一个Transformer，它转换Vector行的数据集，将每个Vector规范化为具有单位范数。 它需要参数p，它指定用于归一化的p范数。（默认情况下p = 2）此标准化有助于标准化输入数据并改善学习算法的行为。</p>
<p><strong>举例</strong></p>
<p>以下示例演示如何以libsvm格式加载数据集，然后将每行标准化为具有单位$\ell^1$范数和单位$\ell^{\infty}$范数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 19:51</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : normalizer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Normalizer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;NormalizerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    dataFrame = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.5</span>, -<span class="number">1.0</span>]),),</span><br><span class="line">        (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">        (<span class="number">2</span>, Vectors.dense([<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">2.0</span>]),)</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L1</span></span><br><span class="line">    normalizer = Normalizer(inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;normFeatures&quot;</span>, p=<span class="number">1.0</span>)</span><br><span class="line">    l1NormData = normalizer.transform(dataFrame)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Normalized using L^1 norm&quot;</span>)</span><br><span class="line">    l1NormData.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># L^&#123;\infty&#125;</span></span><br><span class="line">    lInfNormData = normalizer.transform(dataFrame, &#123;normalizer.p: <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)&#125;)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Normalized using L^inf norm&quot;</span>)</span><br><span class="line">    lInfNormData.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Normalized using L^1 norm</span><br><span class="line">+---+--------------+------------------+</span><br><span class="line">| <span class="built_in">id</span>|      features|      normFeatures|</span><br><span class="line">+---+--------------+------------------+</span><br><span class="line">|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|</span><br><span class="line">|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|</span><br><span class="line">|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|</span><br><span class="line">+---+--------------+------------------+</span><br><span class="line"></span><br><span class="line">Normalized using L^inf norm</span><br><span class="line">+---+--------------+--------------+</span><br><span class="line">| <span class="built_in">id</span>|      features|  normFeatures|</span><br><span class="line">+---+--------------+--------------+</span><br><span class="line">|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|</span><br><span class="line">|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|</span><br><span class="line">|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|</span><br><span class="line">+---+--------------+--------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-14-特征缩放-StandardScaler"><a href="#1-14-特征缩放-StandardScaler" class="headerlink" title="1.14 特征缩放(StandardScaler)"></a>1.14 特征缩放(StandardScaler)</h2><p>StandardScaler将每个特征标准化为具有单位标准差和/或零均值。 它需要参数：</p>
<ul>
<li>withStd：默认为True。 将数据缩放到单位标准偏差。</li>
<li>withMean：默认为False。 在缩放之前使用均值将数据居中。 它将构建密集输出，因此在应用稀疏输入时要小心。</li>
</ul>
<p>StandardScaler是一个Estimator，可以放在数据集上以生成StandardScalerModel; 这等于计算摘要统计。 然后，模型可以将数据集中的“矢量”列(特征)转换为具有单位标准差和/或零均值特征。</p>
<p>请注意，如果要素的标准差为零，则它将在该要素的Vector中返回默认的0.0值。</p>
<p><strong>举例</strong></p>
<p>以下示例演示如何以libsvm格式加载数据集，然后将每个要素标准化以具有单位标准偏差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 20:01</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : standard_scaler_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;StandardScalerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    dataFrame = spark.read.<span class="built_in">format</span>(<span class="string">&quot;libsvm&quot;</span>).load(<span class="string">&quot;../data/mllib/sample_libsvm_data.txt&quot;</span>)</span><br><span class="line">    <span class="comment"># scaler is a Estimator</span></span><br><span class="line">    scaler = StandardScaler(inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;scaledFeatures&quot;</span>, withStd=<span class="literal">True</span>, withMean=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    scalerModel = scaler.fit(dataFrame)  <span class="comment"># Transformer</span></span><br><span class="line"></span><br><span class="line">    scaledData = scalerModel.transform(dataFrame)</span><br><span class="line">    scaledData.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|label|            features|      scaledFeatures|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|  0.0|(692,[127,128,129...|(692,[127,128,129...|</span><br><span class="line">|  1.0|(692,[158,159,160...|(692,[158,159,160...|</span><br><span class="line">|  1.0|(692,[124,125,126...|(692,[124,125,126...|</span><br><span class="line">|  1.0|(692,[152,153,154...|(692,[152,153,154...|</span><br><span class="line">|  1.0|(692,[151,152,153...|(692,[151,152,153...|</span><br><span class="line">|  0.0|(692,[129,130,131...|(692,[129,130,131...|</span><br><span class="line">|  1.0|(692,[158,159,160...|(692,[158,159,160...|</span><br><span class="line">|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|</span><br><span class="line">|  0.0|(692,[154,155,156...|(692,[154,155,156...|</span><br><span class="line">|  0.0|(692,[127,128,129...|(692,[127,128,129...|</span><br><span class="line">|  1.0|(692,[154,155,156...|(692,[154,155,156...|</span><br><span class="line">|  0.0|(692,[153,154,155...|(692,[153,154,155...|</span><br><span class="line">|  0.0|(692,[151,152,153...|(692,[151,152,153...|</span><br><span class="line">|  1.0|(692,[129,130,131...|(692,[129,130,131...|</span><br><span class="line">|  0.0|(692,[154,155,156...|(692,[154,155,156...|</span><br><span class="line">|  1.0|(692,[150,151,152...|(692,[150,151,152...|</span><br><span class="line">|  0.0|(692,[124,125,126...|(692,[124,125,126...|</span><br><span class="line">|  0.0|(692,[152,153,154...|(692,[152,153,154...|</span><br><span class="line">|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|</span><br><span class="line">|  1.0|(692,[124,125,126...|(692,[124,125,126...|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure>
<h2 id="1-15-MinMaxScaler"><a href="#1-15-MinMaxScaler" class="headerlink" title="1.15 MinMaxScaler"></a>1.15 MinMaxScaler</h2><p>MinMaxScaler将每个要素重新缩放到特定范围（通常为[0,1]）。 它需要参数：</p>
<ul>
<li>min：默认为0.0。 转换后的下限，由所有功能共享。</li>
<li>max：默认为1.0。 转换后的上限，由所有功能共享。</li>
</ul>
<p>MinMaxScaler计算数据集的摘要统计信息并生成MinMaxScalerModel。 然后，模型可以单独转换每个特征，使其处于给定范围内。</p>
<p>特征E的重新缩放值计算为，</p>
<script type="math/tex; mode=display">
Rescaled(e_i) = \frac{e_i - E_{min}}{E_{max} - E_{min}} * (max-min) + min</script><p>对于$E_{max} = E_{min}$的情况，$Rescaled(e_i) = 0.5 *(max+min)$的情况</p>
<p>请注意，由于零值可能会转换为非零值，因此即使对于稀疏输入，变压器的输出也将是DenseVector。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 20:08</span></span><br><span class="line"><span class="comment"># @Author   : 01373821 (mingchengyang@sf-express.com)</span></span><br><span class="line"><span class="comment"># @File     : min_max_scaler_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;MinMaxScalerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># 每一行是一个样本，每一列是一个特征</span></span><br><span class="line">    dataFrame = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.1</span>, -<span class="number">1.0</span>]),),</span><br><span class="line">        (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.1</span>, <span class="number">1.0</span>]),),</span><br><span class="line">        (<span class="number">2</span>, Vectors.dense([<span class="number">3.0</span>, <span class="number">10.1</span>, <span class="number">3.0</span>]),)</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    scaler = MinMaxScaler(inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;scaledFeatures&quot;</span>)</span><br><span class="line"></span><br><span class="line">    scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line">    scaledData = scalerModel.transform(dataFrame)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Features scaled to range: [%f, %f]&quot;</span> % (scaler.getMin(), scaler.getMax()))</span><br><span class="line">    scaledData.select(<span class="string">&quot;features&quot;</span>, <span class="string">&quot;scaledFeatures&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Features scaled to range: [0.000000, 1.000000]</span><br><span class="line">+--------------+--------------+</span><br><span class="line">|      features|scaledFeatures|</span><br><span class="line">+--------------+--------------+</span><br><span class="line">|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|</span><br><span class="line">| [2.0,1.1,1.0]| [0.5,0.1,0.5]|</span><br><span class="line">|[3.0,10.1,3.0]| [1.0,1.0,1.0]|</span><br><span class="line">+--------------+--------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-16-MaxAbsScaler"><a href="#1-16-MaxAbsScaler" class="headerlink" title="1.16 MaxAbsScaler"></a>1.16 MaxAbsScaler</h2><p>MaxAbsScaler通过除以每个特征中的最大绝对值，将每个要素重新缩放到范围[-1,1]。 它不会移动/居中数据，因此不会破坏任何稀疏性。</p>
<p>MaxAbsScaler计算数据集的摘要统计信息并生成MaxAbsScalerModel。 然后，模型可以将每个特征单独转换为范围[-1,1]。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 20:44</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : max_abs_scaler_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MaxAbsScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;MaxAbsScalerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    dataFrame = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.1</span>, -<span class="number">8.0</span>]),),</span><br><span class="line">        (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>, -<span class="number">4.0</span>]),),</span><br><span class="line">        (<span class="number">2</span>, Vectors.dense([<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">8.0</span>]),)</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    scaler = MaxAbsScaler(inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;scaledFeatures&quot;</span>)</span><br><span class="line"></span><br><span class="line">    scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line">    scaledData = scalerModel.transform(dataFrame)</span><br><span class="line"></span><br><span class="line">    scaledData.select(<span class="string">&quot;features&quot;</span>, <span class="string">&quot;scaledFeatures&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+--------------+----------------+</span><br><span class="line">|      features|  scaledFeatures|</span><br><span class="line">+--------------+----------------+</span><br><span class="line">|[1.0,0.1,-8.0]|[0.25,0.01,-1.0]|</span><br><span class="line">|[2.0,1.0,-4.0]|  [0.5,0.1,-0.5]|</span><br><span class="line">|[4.0,10.0,8.0]|   [1.0,1.0,1.0]|</span><br><span class="line">+--------------+----------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-17-Bucketizer"><a href="#1-17-Bucketizer" class="headerlink" title="1.17 Bucketizer"></a>1.17 Bucketizer</h2><p>Bucketizer将一列连续特征转换为一列特征桶，其中桶由用户指定。它需要一个参数：</p>
<ul>
<li>splits：用于将连续要素映射到存储桶的参数。对于n + 1个分裂，有n个桶。由分割[x，y)定义的桶保存除最后一个桶之外的[x，y]范围内的值，最后一个桶也包括y。拆分应该严格增加。必须明确提供-inf，inf处的值以涵盖所有Double值;否则，指定的拆分之外的值将被视为错误。分裂的两个例子是Array（Double.NegativeInfinity，0.0, 1.0，Double.PositiveInfinity）和Array（0.0, 1.0, 2.0）。</li>
</ul>
<p>请注意，如果您不知道目标列的上限和下限，则应添加Double.NegativeInfinity和Double.PositiveInfinity作为拆分的边界，以防止可能超出Bucketizer边界异常。</p>
<p>另请注意，您提供的分割必须严格按顺序递增，即s0 &lt;s1 &lt;s2 &lt;… &lt;sn。</p>
<p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/7/31 20:49</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : bucketizer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Bucketizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;BucketizerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    splits = [-<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>), -<span class="number">0.5</span>, <span class="number">0.0</span>, <span class="number">0.5</span>, <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)]</span><br><span class="line"></span><br><span class="line">    data = [(-<span class="number">999.9</span>,), (-<span class="number">0.5</span>,), (-<span class="number">0.3</span>,), (<span class="number">0.0</span>,), (<span class="number">0.2</span>,), (<span class="number">999.9</span>,)]</span><br><span class="line">    dataFrame = spark.createDataFrame(data, [<span class="string">&quot;features&quot;</span>])</span><br><span class="line"></span><br><span class="line">    bucketizer = Bucketizer(splits=splits, inputCol=<span class="string">&quot;features&quot;</span>, outputCol=<span class="string">&quot;bucketedFeatures&quot;</span>)</span><br><span class="line"></span><br><span class="line">    bucketedData = bucketizer.transform(dataFrame)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Bucketizer output with %d buckets&quot;</span> % (<span class="built_in">len</span>(bucketizer.getSplits())-<span class="number">1</span>))</span><br><span class="line">    bucketedData.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Bucketizer output with 4 buckets</span><br><span class="line">+--------+----------------+</span><br><span class="line">|features|bucketedFeatures|</span><br><span class="line">+--------+----------------+</span><br><span class="line">|  -999.9|             0.0|</span><br><span class="line">|    -0.5|             1.0|</span><br><span class="line">|    -0.3|             1.0|</span><br><span class="line">|     0.0|             2.0|</span><br><span class="line">|     0.2|             2.0|</span><br><span class="line">|   999.9|             3.0|</span><br><span class="line">+--------+----------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-18-向量内积-ElementwiseProduct"><a href="#1-18-向量内积-ElementwiseProduct" class="headerlink" title="1.18 向量内积(ElementwiseProduct)"></a>1.18 向量内积(ElementwiseProduct)</h2><p>ElementwiseProduct使用基于元素的乘法将每个输入向量乘以提供的“权重”向量。 换句话说，它通过标量乘数来缩放数据集的每一列。 这表示输入矢量v和变换矢量w之间的Hadamard乘积，以产生结果矢量。</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
v_1 \\
\vdots \\
v_N
\end{pmatrix} \circ \begin{pmatrix}
                    w_1 \\
                    \vdots \\
                    w_N
                    \end{pmatrix}
= \begin{pmatrix}
  v_1 w_1 \\
  \vdots \\
  v_N w_N
  \end{pmatrix}</script><p><strong>举例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/8/1 15:30</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : elementwise_product_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> ElementwiseProduct</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;ElementwiseProductExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    data = [(Vectors.dense([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]),), (Vectors.dense([<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]),)]</span><br><span class="line">    df = spark.createDataFrame(data, [<span class="string">&quot;vector&quot;</span>])</span><br><span class="line">    transformer = ElementwiseProduct(scalingVec=Vectors.dense([<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>]),</span><br><span class="line">                                     inputCol=<span class="string">&quot;vector&quot;</span>, outputCol=<span class="string">&quot;transformedVector&quot;</span>)</span><br><span class="line">    transformer.transform(df).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+-------------+-----------------+</span><br><span class="line">|       vector|transformedVector|</span><br><span class="line">+-------------+-----------------+</span><br><span class="line">|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|</span><br><span class="line">|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|</span><br><span class="line">+-------------+-----------------+</span><br></pre></td></tr></table></figure>
<h2 id="1-19-SQLTransformer"><a href="#1-19-SQLTransformer" class="headerlink" title="1.19 SQLTransformer"></a>1.19 SQLTransformer</h2><p>SQLTransformer实现由SQL语句定义的转换。 目前我们只支持SQL语法，如“SELECT … FROM <strong>THIS</strong> …”，其中“<strong>THIS</strong>”表示输入数据集的基础表。 select子句指定要在输出中显示的字段，常量和表达式，并且可以是Spark SQL支持的任何select子句。 用户还可以使用Spark SQL内置函数和UDF对这些选定列进行操作。 例如，SQLTransformer支持如下语句：</p>
<ul>
<li>SELECT a，a + b AS a_b FROM <strong>THIS</strong></li>
<li>SELECT a，SQRT（b）AS b_sqrt FROM <strong>THIS</strong> WHERE a &gt; 5</li>
<li>SELECT a，b，SUM（c）AS c_sum FROM <strong>THIS</strong> GROUP BY a，b</li>
</ul>
<p><strong>举例</strong></p>
<p>假设我们有以下具有列id，v1和v2的DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> |  v1 |  v2</span><br><span class="line">----|-----|-----</span><br><span class="line"> 0  | 1.0 | 3.0  </span><br><span class="line"> 2  | 2.0 | 5.0</span><br></pre></td></tr></table></figure>
<p>这是SQLTransformer的输出，其语句为“SELECT <em>，（v1 + v2）AS v3，（v1 </em> v2）AS v4 FROM <strong>THIS</strong>”：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+---+---+---+---+----+</span><br><span class="line">| <span class="built_in">id</span>| v1| v2| v3|  v4|</span><br><span class="line">+---+---+---+---+----+</span><br><span class="line">|  0|1.0|3.0|4.0| 3.0|</span><br><span class="line">|  2|2.0|5.0|7.0|10.0|</span><br><span class="line">+---+---+---+---+----+</span><br></pre></td></tr></table></figure>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/8/1 15:35</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : sql_transformer.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> SQLTransformer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;SQLTransformerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame([</span><br><span class="line">        (<span class="number">0</span>, <span class="number">1.0</span>, <span class="number">3.0</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">2.0</span>, <span class="number">5.0</span>)</span><br><span class="line">    ], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;v1&quot;</span>, <span class="string">&quot;v2&quot;</span>])</span><br><span class="line">    sqlTrans = SQLTransformer(</span><br><span class="line">        statement=<span class="string">&quot;SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__&quot;</span>)</span><br><span class="line">    sqlTrans.transform(df).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="1-20-矢量汇编-VectorAssembler"><a href="#1-20-矢量汇编-VectorAssembler" class="headerlink" title="1.20 矢量汇编(VectorAssembler)"></a>1.20 矢量汇编(VectorAssembler)</h2><p>VectorAssembler是一个Transformer，它将给定的字段列表组合到一个向量列中。 将原始特征和由不同特征变换器生成的特征组合成单个特征向量非常有用，以便训练ML模型，如逻辑回归和决策树。 VectorAssembler接受以下输入列类型：所有数字类型，布尔类型和矢量类型。 在每一行中，输入列的值将按指定的顺序连接到一个向量中。</p>
<p><strong>举例</strong></p>
<p>假设我们有一个带有id，hour，mobile，userFeatures和clicked列的DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> | hour | mobile | userFeatures     | clicked</span><br><span class="line">----|------|--------|------------------|---------</span><br><span class="line"> 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0</span><br></pre></td></tr></table></figure>
<p>userFeatures是一个包含三个用户特征的矢量列。 我们希望将hour，mobile和userFeatures组合成一个单个的特征向量，并使用它来预测被点击与否。 如果我们将VectorAssembler的输入列设置为hour，mobile和userFeatures，并将输出列设置为features，转换后我们应该得到以下DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------+-------+</span><br><span class="line">|features               |clicked|</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">|[18.0,1.0,0.0,10.0,0.5]|1.0    |</span><br><span class="line">+-----------------------+-------+</span><br></pre></td></tr></table></figure>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/8/1 15:40</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : vector_assembler_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;VectorAssemblerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    dataset = spark.createDataFrame(</span><br><span class="line">        [(<span class="number">0</span>, <span class="number">18</span>, <span class="number">1.0</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">0.5</span>]), <span class="number">1.0</span>)],</span><br><span class="line">        [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;hour&quot;</span>, <span class="string">&quot;mobile&quot;</span>, <span class="string">&quot;userFeatures&quot;</span>, <span class="string">&quot;clicked&quot;</span>])</span><br><span class="line"></span><br><span class="line">    assembler = VectorAssembler(</span><br><span class="line">        inputCols=[<span class="string">&quot;hour&quot;</span>, <span class="string">&quot;mobile&quot;</span>, <span class="string">&quot;userFeatures&quot;</span>],</span><br><span class="line">        outputCol=<span class="string">&quot;features&quot;</span>)</span><br><span class="line"></span><br><span class="line">    output = assembler.transform(dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Assembled columns &#x27;hour&#x27;, &#x27;mobile&#x27;, &#x27;userFeatures&#x27; to vector column &#x27;features&#x27;&quot;</span>)</span><br><span class="line">    output.select(<span class="string">&quot;features&quot;</span>, <span class="string">&quot;clicked&quot;</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="1-21-矢量大小提示-VectorSizeHint"><a href="#1-21-矢量大小提示-VectorSizeHint" class="headerlink" title="1.21 矢量大小提示(VectorSizeHint)"></a>1.21 矢量大小提示(VectorSizeHint)</h2><p>有时可以明确指定VectorType列的向量大小。例如，VectorAssembler使用其输入列中的大小信息来为其输出列生成大小信息和元数据。虽然在某些情况下可以通过检查列的内容来获得此信息，但是在流数据中，在流启动之前内容不可用。 VectorSizeHint允许用户显式指定列的向量大小，以便VectorAssembler或可能需要知道向量大小的其他变换器可以将该列用作输入。</p>
<p>要使用VectorSizeHint，用户必须设置inputCol和size参数。将此转换器应用于dataframe会生成一个新的dataframe，其中包含inputCol的更新元数据，用于指定矢量大小。结果数据流的下游操作可以使用meatadata获得此大小。</p>
<p>VectorSizeHint还可以使用一个可选的handleInvalid参数，该参数在向量列包含空值或大小错误的向量时控制其行为。默认情况下，handleInvalid设置为“error”，表示应该抛出异常。此参数也可以设置为“skip”，表示应该从结果数据帧中过滤掉包含无效值的行，或“optimistic”，表示不应检查列是否存在无效值，并且应保留所有行。请注意，使用“optimistic”会导致生成的数据流处于不一致状态，应用VectorSizeHint列的元数据与该列的内容不匹配。用户应注意避免这种不一致的状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/8/1 15:47</span></span><br><span class="line"><span class="comment"># @Author   : 01373821 (mingchengyang@sf-express.com)</span></span><br><span class="line"><span class="comment"># @File     : vector_size_hint_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> (VectorSizeHint, VectorAssembler)</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;VectorSizeHintExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    dataset = spark.createDataFrame(</span><br><span class="line">        [(<span class="number">0</span>, <span class="number">18</span>, <span class="number">1.0</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">0.5</span>]), <span class="number">1.0</span>),</span><br><span class="line">         (<span class="number">0</span>, <span class="number">18</span>, <span class="number">1.0</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">10.0</span>]), <span class="number">0.0</span>)],</span><br><span class="line">        [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;hour&quot;</span>, <span class="string">&quot;mobile&quot;</span>, <span class="string">&quot;userFeatures&quot;</span>, <span class="string">&quot;clicked&quot;</span>])</span><br><span class="line"></span><br><span class="line">    sizeHint = VectorSizeHint(</span><br><span class="line">        inputCol=<span class="string">&quot;userFeatures&quot;</span>,</span><br><span class="line">        handleInvalid=<span class="string">&quot;skip&quot;</span>,</span><br><span class="line">        size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    datasetWithSize = sizeHint.transform(dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Rows where &#x27;userFeatures&#x27; is not the right size are filtered out&quot;</span>)</span><br><span class="line">    datasetWithSize.show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    assembler = VectorAssembler(</span><br><span class="line">        inputCols=[<span class="string">&quot;hour&quot;</span>, <span class="string">&quot;mobile&quot;</span>, <span class="string">&quot;userFeatures&quot;</span>],</span><br><span class="line">        outputCol=<span class="string">&quot;features&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该数据流可以用于下游的Transformers</span></span><br><span class="line">    output = assembler.transform(datasetWithSize)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Assembled columns &#x27;hour&#x27;, &#x27;mobile&#x27;, &#x27;userFeatures&#x27; to vector column &#x27;features&#x27;&quot;</span>)</span><br><span class="line">    output.select(<span class="string">&quot;features&quot;</span>, <span class="string">&quot;clicked&quot;</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Rows <span class="built_in">where</span> <span class="string">&#x27;userFeatures&#x27;</span> is not the right size are filtered out</span><br><span class="line">+---+----+------+--------------+-------+</span><br><span class="line">|<span class="built_in">id</span> |hour|mobile|userFeatures  |clicked|</span><br><span class="line">+---+----+------+--------------+-------+</span><br><span class="line">|0  |18  |1.0   |[0.0,10.0,0.5]|1.0    |</span><br><span class="line">+---+----+------+--------------+-------+</span><br><span class="line"></span><br><span class="line">Assembled columns <span class="string">&#x27;hour&#x27;</span>, <span class="string">&#x27;mobile&#x27;</span>, <span class="string">&#x27;userFeatures&#x27;</span> to vector column <span class="string">&#x27;features&#x27;</span></span><br><span class="line">+-----------------------+-------+</span><br><span class="line">|features               |clicked|</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">|[18.0,1.0,0.0,10.0,0.5]|1.0    |</span><br><span class="line">+-----------------------+-------+</span><br></pre></td></tr></table></figure>
<h2 id="1-22-分位数离散化器-QuantileDiscretizer"><a href="#1-22-分位数离散化器-QuantileDiscretizer" class="headerlink" title="1.22 分位数离散化器(QuantileDiscretizer)"></a>1.22 分位数离散化器(QuantileDiscretizer)</h2><p>QuantileDiscretizer采用具有连续特征的列，并输出具有分箱分类特征的列。 bin的数量由numBuckets参数设置。所使用的桶的数量可能小于该值，例如，如果输入的不同值太少而不能创建足够的不同分位数。</p>
<p>NaN值：在QuantileDiscretizer拟合期间，NaN值将从中移除。这将产生用于进行预测的Bucketizer模型。在转换过程中，Bucketizer会在数据集中找到NaN值时引发错误，但用户也可以选择通过设置handleInvalid来保留或删除数据集中的NaN值。如果用户选择保留NaN值，它们将被专门处理并放入自己的桶中，例如，如果使用4个桶，那么非NaN数据将被放入桶[0-3]，但是NaN将是算在一个特殊的桶[4]。</p>
<p>算法：使用近似算法选择bin范围（有关详细说明，请参阅<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.3.2/api/scala/index.html">approxQuantile</a>的文档）。可以使用relativeError参数控制近似的精度。设置为零时，计算精确分位数（注意：计算精确分位数是一项昂贵的操作）。下边界和上边界将是-Infinity和+ Infinity，覆盖所有实际值。</p>
<p><strong>举例</strong></p>
<p>假设我们有一个包含列id，小时的DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">id</span> | hour</span><br><span class="line">----|------</span><br><span class="line"> 0  | 18.0</span><br><span class="line">----|------</span><br><span class="line"> 1  | 19.0</span><br><span class="line">----|------</span><br><span class="line"> 2  | 8.0</span><br><span class="line">----|------</span><br><span class="line"> 3  | 5.0</span><br><span class="line">----|------</span><br><span class="line"> 4  | 2.2</span><br></pre></td></tr></table></figure>
<p>小时是Double类型的连续特征。 我们希望将连续特征变为分类特征。 给定numBuckets = 3，我们应该得到以下DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+---+----+------+</span><br><span class="line">| <span class="built_in">id</span>|hour|result|</span><br><span class="line">+---+----+------+</span><br><span class="line">|  0|18.0|   2.0|</span><br><span class="line">|  1|19.0|   2.0|</span><br><span class="line">|  2| 8.0|   1.0|</span><br><span class="line">|  3| 5.0|   1.0|</span><br><span class="line">|  4| 2.2|   0.0|</span><br><span class="line">+---+----+------+</span><br></pre></td></tr></table></figure>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/8/1 15:53</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : quantile_discretizer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> QuantileDiscretizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;QuantileDiscretizerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    data = [(<span class="number">0</span>, <span class="number">18.0</span>), (<span class="number">1</span>, <span class="number">19.0</span>), (<span class="number">2</span>, <span class="number">8.0</span>), (<span class="number">3</span>, <span class="number">5.0</span>), (<span class="number">4</span>, <span class="number">2.2</span>)]</span><br><span class="line">    df = spark.createDataFrame(data, [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;hour&quot;</span>])</span><br><span class="line">    df = df.repartition(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    discretizer = QuantileDiscretizer(numBuckets=<span class="number">3</span>, inputCol=<span class="string">&quot;hour&quot;</span>, outputCol=<span class="string">&quot;result&quot;</span>)</span><br><span class="line"></span><br><span class="line">    result = discretizer.fit(df).transform(df)</span><br><span class="line">    result.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="1-23-Imputer"><a href="#1-23-Imputer" class="headerlink" title="1.23 Imputer"></a>1.23 Imputer</h2><p>Imputer转换器使用缺失值所在的列的平均值或中值来完成数据集中的缺失值。 输入列应为DoubleType或FloatType。 目前，Imputer不支持分类功能，并且可能为包含分类功能的列创建不正确的值。 Imputer可以通过.setMissingValue（custom_value）将“NaN”以外的自定义值包括在内。 例如，.setMissingValue（0）将计算所有出现的（0）。</p>
<p><strong>注意</strong>，输入列中的所有空值都被视为缺失，因此也会被估算。</p>
<p><strong>举例</strong></p>
<p>假设我们有一个包含a和b列的DataFrame：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">      a     |      b      </span><br><span class="line">------------|-----------</span><br><span class="line">     1.0    | Double.NaN</span><br><span class="line">     2.0    | Double.NaN</span><br><span class="line"> Double.NaN |     3.0   </span><br><span class="line">     4.0    |     4.0   </span><br><span class="line">     5.0    |     5.0 </span><br></pre></td></tr></table></figure>
<p>在此示例中，Imputer将使用从相应列中的其他值计算的均值（默认插补策略）替换所有出现的Double.NaN（缺失值的缺省值）。 在此示例中，列a和b的替代值分别为3.0和4.0。 转换后，输出列中的缺失值将替换为相关列的替代值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+---+---+-----+-----+</span><br><span class="line">|  a|  b|out_a|out_b|</span><br><span class="line">+---+---+-----+-----+</span><br><span class="line">|1.0|NaN|  1.0|  4.0|</span><br><span class="line">|2.0|NaN|  2.0|  4.0|</span><br><span class="line">|NaN|3.0|  3.0|  3.0|</span><br><span class="line">|4.0|4.0|  4.0|  4.0|</span><br><span class="line">|5.0|5.0|  5.0|  5.0|</span><br><span class="line">+---+---+-----+-----+</span><br></pre></td></tr></table></figure>
<p>示例代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2019/8/1 15:59</span></span><br><span class="line"><span class="comment"># @Author   : buracagyang</span></span><br><span class="line"><span class="comment"># @File     : imputer_example.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Describe:</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;ImputerExample&quot;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)),</span><br><span class="line">        (<span class="number">2.0</span>, <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)),</span><br><span class="line">        (<span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>), <span class="number">3.0</span>),</span><br><span class="line">        (<span class="number">4.0</span>, <span class="number">4.0</span>),</span><br><span class="line">        (<span class="number">5.0</span>, <span class="number">5.0</span>)</span><br><span class="line">    ], [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>])</span><br><span class="line"></span><br><span class="line">    imputer = Imputer(inputCols=[<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], outputCols=[<span class="string">&quot;out_a&quot;</span>, <span class="string">&quot;out_b&quot;</span>])</span><br><span class="line">    model = imputer.fit(df)</span><br><span class="line">    model.transform(df).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><div class="article-licensing box"><div class="licensing-title"><p>【Spark】特征工程2-Transformers</p><p><a href="http://example.com/2019/07/31/spark-features-project-2/">http://example.com/2019/07/31/spark-features-project-2/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Buracag</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2019-07-31</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2019-08-09</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/">技术备忘</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/08/02/spark-classification-and-regression-1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">【Spark】分类和回归算法-分类</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/07/30/spark-features-project-1/"><span class="level-item">【Spark】特征工程1-Extractors</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "50275a5f1f3b5d8ab8650959a5d2c4d5",
            repo: "blog_comments",
            owner: "buracagyang",
            clientID: "b59e8ee91a1328df0e4b",
            clientSecret: "e8a8251bb9bd5ec9913a699734a6824bea1d6ec3",
            admin: "buracagyang",
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/me.png" alt="Buracag"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Buracag</p><p class="is-size-6 is-block">数据挖掘与分析工程师</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>SHENZHEN,CN</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">35</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">9</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/buracagyang" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/buracagyang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-01-15T05:13:57.000Z">2020-01-15</time></p><p class="title"><a href="/2020/01/15/graph-embedding-struc2vec/">【Graph Embedding】struc2vec</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-01-14T05:55:09.000Z">2020-01-14</time></p><p class="title"><a href="/2020/01/14/graph-embedding-SDNE/">【Graph Embedding】SDNE</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-01-05T11:36:25.000Z">2020-01-05</time></p><p class="title"><a href="/2020/01/05/graph-embedding-node2vec/">【Graph Embedding】node2vec</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2019-12-26T08:16:26.000Z">2019-12-26</time></p><p class="title"><a href="/2019/12/26/graph-embedding-deepwalk/">【Graph Embedding】DeepWalk</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2019-12-21T09:12:54.000Z">2019-12-21</time></p><p class="title"><a href="/2019/12/21/graph-embedding-line/">【Graph Embedding】line</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">December 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">August 2019</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">July 2019</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/06/"><span class="level-start"><span class="level-item">June 2019</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">May 2019</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Embedding/"><span class="tag">Embedding</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%A1%E7%AE%97/"><span class="tag">图计算</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="tag">基础知识</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"><span class="tag">大数据</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"><span class="tag">技术备忘</span><span class="tag">28</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"><span class="tag">算法备忘</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"><span class="tag">统计学运用</span><span class="tag">4</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#1-特征转换器"><span class="level-left"><span class="level-item">1. 特征转换器</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-分词器-Tokenizer"><span class="level-left"><span class="level-item">1.1 分词器(Tokenizer)</span></span></a></li><li><a class="level is-mobile" href="#1-2-StopWordsRemover"><span class="level-left"><span class="level-item">1.2 StopWordsRemover</span></span></a></li><li><a class="level is-mobile" href="#1-3-n-gram"><span class="level-left"><span class="level-item">1.3 n-gram</span></span></a></li><li><a class="level is-mobile" href="#1-4-二元化-Binarizer"><span class="level-left"><span class="level-item">1.4 二元化(Binarizer)</span></span></a></li><li><a class="level is-mobile" href="#1-5-主成分分析-PCA"><span class="level-left"><span class="level-item">1.5 主成分分析(PCA)</span></span></a></li><li><a class="level is-mobile" href="#1-6-多项式扩展-PolynomialExpansion"><span class="level-left"><span class="level-item">1.6 多项式扩展(PolynomialExpansion)</span></span></a></li><li><a class="level is-mobile" href="#1-7-离散余弦距离-Discrete-Cosine-Transform-DCT"><span class="level-left"><span class="level-item">1.7 离散余弦距离(Discrete Cosine Transform, DCT)</span></span></a></li><li><a class="level is-mobile" href="#1-8-字符串索引器-StringIndexer"><span class="level-left"><span class="level-item">1.8 字符串索引器(StringIndexer)</span></span></a></li><li><a class="level is-mobile" href="#1-9-IndexToString"><span class="level-left"><span class="level-item">1.9 IndexToString</span></span></a></li><li><a class="level is-mobile" href="#1-10-One-Hot-OneHotEncoderEstimator"><span class="level-left"><span class="level-item">1.10 One-Hot(OneHotEncoderEstimator)</span></span></a></li><li><a class="level is-mobile" href="#1-11-矢量索引器-VectorIndexer"><span class="level-left"><span class="level-item">1.11 矢量索引器(VectorIndexer)</span></span></a></li><li><a class="level is-mobile" href="#1-12-交互作用-Interaction"><span class="level-left"><span class="level-item">1.12 交互作用(Interaction)</span></span></a></li><li><a class="level is-mobile" href="#1-13-标准化-Normalizer"><span class="level-left"><span class="level-item">1.13 标准化(Normalizer)</span></span></a></li><li><a class="level is-mobile" href="#1-14-特征缩放-StandardScaler"><span class="level-left"><span class="level-item">1.14 特征缩放(StandardScaler)</span></span></a></li><li><a class="level is-mobile" href="#1-15-MinMaxScaler"><span class="level-left"><span class="level-item">1.15 MinMaxScaler</span></span></a></li><li><a class="level is-mobile" href="#1-16-MaxAbsScaler"><span class="level-left"><span class="level-item">1.16 MaxAbsScaler</span></span></a></li><li><a class="level is-mobile" href="#1-17-Bucketizer"><span class="level-left"><span class="level-item">1.17 Bucketizer</span></span></a></li><li><a class="level is-mobile" href="#1-18-向量内积-ElementwiseProduct"><span class="level-left"><span class="level-item">1.18 向量内积(ElementwiseProduct)</span></span></a></li><li><a class="level is-mobile" href="#1-19-SQLTransformer"><span class="level-left"><span class="level-item">1.19 SQLTransformer</span></span></a></li><li><a class="level is-mobile" href="#1-20-矢量汇编-VectorAssembler"><span class="level-left"><span class="level-item">1.20 矢量汇编(VectorAssembler)</span></span></a></li><li><a class="level is-mobile" href="#1-21-矢量大小提示-VectorSizeHint"><span class="level-left"><span class="level-item">1.21 矢量大小提示(VectorSizeHint)</span></span></a></li><li><a class="level is-mobile" href="#1-22-分位数离散化器-QuantileDiscretizer"><span class="level-left"><span class="level-item">1.22 分位数离散化器(QuantileDiscretizer)</span></span></a></li><li><a class="level is-mobile" href="#1-23-Imputer"><span class="level-left"><span class="level-item">1.23 Imputer</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2022 Buracag</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>